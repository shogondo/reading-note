# Chapter 11. Understanding Kubernetes internals

* この章のまとめ
  * Kubernetes クラスタを構成するコンポーネントとそれらの役割
  * API サーバーやスケジューラ、各種コントローラ、Kubelet がどのように動作するのか
  * インフラストラクチャ Pod がどのように Pod 内のコンテナ間のバインドを行うか
  * Pod が同一ノード内やノード間でどのように通信しているのか
  * kube-proxy が iptables を使ってどのように Pod のロードバランシングを行なっているのか
  * クラスタの HA を実現するためにコントロールプレーンのコンポーネントが複数のインスタンスでどのように動作しているのか

## 11.1. UNDERSTANDING THE ARCHITECTURE

おさらい。Kubernetes クラスタは次のようなコンポーネントからなる。

* コントロールプレーン
  * etcd
  * API サーバー
  * スケジューラ
  * コントローラマネージャ
* ワーカーノード
  * Kubelet
  * Kubernetes サービスプロキシ (kube-proxy)
  * コンテナランタイム (Docker, rkt など)
* アドオン
  * Kubernetes DNS サーバー
  * ダッシュボード
  * Ingress コントローラ
  * Heapster (14 章で取り上げる)
  * CNI ネットワークプラグイン (この章で後ほど取り上げる)

### 11.1.1. The distributed nature of Kubernetes components

* 各コンポーネントはそれぞれ独立したプロセスとして実行される。コンポーネントとそれらの依存関係の概要が次の図。

* ![11fig01](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig01.jpg)

* Kubernetes のシステムコンポーネントは直接やりとりせず、API サーバーのみを使ってコミュニケーションする。
* API サーバーは etcd と直接通信可能な唯一のコンポーネント。
* その他のコンポーネントは API サーバーを介してクラスタの状態を更新する。
* コントロールプレーンのコンポーネントは複数のホストに分割して稼働させ、高可用性を実現することができる。
* etcd と API サーバーは複数のホストでパラレルに実行可能。
* スケジューラやコントローラマネージャはアクティブ・スタンバイ構成になる。
* Kubelet は常に通常のシステムコンポーネントとして稼働させ必要があるが、コントロールプレーンのコンポーネントと kube-proxy は Pod として稼働させることも可能。
* コントロールプレーンのコンポーネントを Pod 化したい場合は、Kubelet もマスターにデプロイする必要がある。

### 11.1.2. How Kubernetes uses etcd

* Kubernetes はクラスタの状態やメタデータを永続化するために etcd を使用している
* etcd に唯一直接アクセス可能なのが API サーバーで、その他のコンポーネントは API サーバー経由で間接的にアクセスする
* そのため、一貫した楽観的排他制御やバリデーション、永続化方法の抽象化が可能
* Kubernetes リソースは metadata.resourceVersion フィールドを含んでいて、これで楽観的排他制御を行なっている
* この書籍執筆時点では、Kubernetes では etcd v2 と v3 が利用可能だが、パフォーマンス改善が施されている v3 の利用が推奨される
  * EKS はバージョン 3 が使われている
  * https://tiny.amazon.com/13d73c9qg/wamazindeAmazDeveEKS
  * EKS is running the recommended version of etcd, so it isn't vulnerable to this data corruption bug from the Kubernetes etcd recommendations: https://discuss.kubernetes.io/t/recommended-etcd-minimum-versions-3-1-11-3-2-10-3-3-0/2637
* Kubernetes は /registry というキープレフィックス配下に全てのデータを格納する。

```:bash
$ etcdctl ls /registry
/registry/configmaps
/registry/daemonsets
/registry/deployments
/registry/events
/registry/namespaces
/registry/pods
...
```

* Pod の場合は、/registry/pods/<名前空間>/<Pod 名> のようなキーに情報が格納される。
* 格納されるのは Pod 定義の JSON そのもの。

```
$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga
{"kind":"Pod","apiVersion":"v1","metadata":{"name":"kubia-159041347-wt6ga",
"generateName":"kubia-159041347-","namespace":"default","selfLink":...
```

* etcd をマルチインスタンス構成にすることで可用性を高めることができる
* 一方、一貫した状態を保つためにマルチインスタンス間でコンセンサスが必要
* etcd は RAFT コンセンサスアルゴリズムを使う
* RAFT では、状態遷移をさせるには過半数のノードの同意が必要
* ノード群が二つのグループにネットワーク的に分断されても、それぞれのグループの状態が分岐して別々に遷移してしまうことはない
* 状態遷移可能なのは過半数のノードが参加しているグループのみ
* ネットワーク分断が解消された際に、過半数でなかったグループのノードがもう一方のグループの状態に同期される

**RAFT について調べる**

* ![11fig02_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig02_alt.jpg)

* etcd は一般的に奇数のインスタンスにデプロイされる
* 例えば etcd が 2 インスタンス構成の場合、片方のインスタンスで問題が発生した時点で状態遷移ができなくなる (過半数の同意、が得られなくなるので)
* 通常、大きなクラスタでは etcd は 5 から 7 ノードで十分 (2 or 3 ノードの問題に対応できる)
  * EKS
  * https://w.amazon.com/index.php/Albduque/Services/EKS
  * Provisions 3 Masters and 3 Etcd backends across 3 AZs

### 11.1.3. What the API server does

* ![11fig01](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig01.jpg)

* API サーバーは他のコンポーネントやクライアント (kubectl など) から利用される中心的なコンポーネントで、これらに対して RESTFul API を提供する
* 状態を etcd に保存し、バリデーションや楽観的排他制御を担当している
* 例えば kubectl でリソース作成を行なった場合、HTTP POST でマニフェストを API サーバーに送信され、下図のような流れで処理が行われる。

* ![11fig03_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig03_alt.jpg)

* まず、Authentication プラグインによってリクエストの認証が行われる
* APIサーバーは、誰がリクエストを送信したのかを判断するまで、これらのプラグインを順番に呼び出す
* 次に Authorization プラグインによって、認証されたユーザーがリクエストされたリソースに対するアクションを行う権限があるのか確認する
* リクエストがリソースの作成、更新、削除なら次に Admission Control プラグインが実行される
* Admission Control プラグインによって、リソース仕様から欠けているフィールドをデフォルト値に初期化する、リクエストを拒否する、といったことが発生する
* リクエストが読み取りしか行わない場合は Admission Control プラグインは実行されない
* Admission Control プラグインの例
  * AlwaysPullImages: Pod の imagePullPolicy を Always に上書きする (常にイメージを pull することを強制する)
  * ServiceAccount: Pod にデフォルトサービスアカウントを適用する
  * NamespaceLifecycle: 削除中や存在しないネームスペースに Pod が作成されるのを防ぐ
  * ResourceQuota: 特定のネームスペース内の Pod が、ネームスペースに割り当てられているのと同じだけの CPU とメモリを使用するようにする。
  * その他の Admission Control プラグインについては https://kubernetes.io/docs/admin/admission-controllers/ を参照
* リクエストが全ての Admission Control を通過したら、API サーバーはバリデーションを行い、データを etcd に格納する

### 11.1.4. Understanding how the API server notifies clients of resource changes

* API サーバーは上記以外のことは行わない。
* つまり、Pod を作ったりするのは API サーバーの役割ではない。
* API サーバーは他のコンポーネントに対してリソースの変更を検知できる機能を提供する
* API サーバー以外のコンポーネントは、クラスタメタデータの変更を検知して、何か処理を行うことができる

* ![11fig04_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig04_alt.jpg)

* kubectl でも変更を subscribe することが可能 (--watch オプション)
* 例えば `kubectl get pods --watch` しながら Pod を作成した場合、Pod のステータス等に変更があるたびにその状態がコンソールに出力されるような動作になる。

### 11.1.5. Understanding the Scheduler

* ![11fig01](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig01.jpg)

* スケジューラは、どのノードで Pod を稼働させるかを選択するコンポーネント。
* スケジューラが実行するのは、API サーバーを介して Pod 定義を更新することだけ (選択したノードに Pod を実行するように直接指示するわけではない)。
* API サーバーから変更を watch している Kubelet に対してその変更の通知が送られ、Pod が起動されるという流れになる。

* ノードの選択は大きく分けると二つのパートからなる
  * Pod をスケジュールすることが可能なノードのみをフィルタする
  * その中から最も適切なノードを優先させる (同じプライオリティのノードが複数あればラウンドロビンで選択)

* ![11fig05_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig05_alt.jpg)

* 以下のような観点でスケジューラは Pod をスケジュールすることが可能なノードのみをフィルタする
  * Pod のハードウェア要求を満たすノードか
  * ノードのリソースが足りているか
  * Pod のメタデータで by name で指定されたノードか
  * ノードセレクタにマッチするラベルを持っているか
  * Pod がバインドを要求しているポートがすでに使われているか
  * Pod が要求しているボリュームを使えるか (ノードがマウントしていて、他の Pod が使っていないか)
  * Taints/Tolerations や Affinity のルール

* 各ノードに対して上のチェックを行い、Pod をスケジュール可能なノードを洗い出す

* スケジュール可能なノードが複数ある場合にどのノードを選択すべきかは、複数のファクターが絡むなかなか難しい問題
  * ノードが二つあり、一方のノードで Pod が動いている状態でもう一つ起動する場合は、もう一方のノードで起動してほしい
  * だが、クラウドインフラでノードを起動しているのなら、一方のノードに Pod をまとめてもう一つのノードを停止してコストを節約する方がベターかもしれない。
  * でも複数の Pod のレプリカを起動している状況なら、ノード障害時の影響を抑えるためできるだけ複数ノードに分散してほしい
* そのため、スケジューラはニーズに合わせて設定したり、独自実装に置き換えたりできる
* 複数のスケジューラを使うことも可能。
* Pod にスケジューラ名を指定して、どのスケジューラでスケジュールさせるのか指定できる

### 11.1.6. Introducing the controllers running in the Controller Manager

* ![11fig01](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig01.jpg)

* API サーバーはリソースの情報を etcd に格納してクライアントに通知するだけ、スケジューラは Pod をノードにアサインするだけ。
* クラスタを期待された状態 (etcd に格納されているリソースの状態) にするためのコンポーネントが必要 => これがコントローラマネージャ内で稼働しているコントローラ
* 現在は単一のコントローラマネージャプロセスが各コントローラの処理を実行しているが、最終的にはコントローラごとに別々のプロセスとし、カスタム実装で置き換えられるようになる
  * まだそうなってなさそう
  * https://kubernetes.io/docs/concepts/overview/components/
  * Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.
* このようなコントローラがある
  * Replication Manager (a controller for ReplicationController resources)
  * ReplicaSet, DaemonSet, and Job controllers
  * Deployment controller
  * StatefulSet controller
  * Node controller
  * Service controller
  * Endpoints controller
  * Namespace controller
  * PersistentVolume controller

* コントローラは API サーバーの変更を watch し、リソースの作成や更新を検知してクラスタをその状態にするための処理を行う
* 基本的には watch を使っているが、全てのイベントが拾えることが保証されていないので、何か処理漏れがないかを確認するため定期的なポーリングも合わせて行なっている
* コントローラ同士はお互いに直接通信しない (お互いの存在すら知らない)
* コントローラのコード https://github.com/kubernetes/kubernetes/tree/master/pkg/controller

<!-- #### Replication Manager

* API サーバーを watch して Pod 数が期待するレプリカ数になるように調整する
* Replication Manager は直接 Pod を起動しない。Pod 数が足りないことを検知したら API サーバーに Pod リソースを作成し、スケジューラと Kubelet がそれを検知して Pod を起動する
* ![11fig06_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig06_alt.jpg)

#### Node controller

* 実際に稼働しているノードとノードリソースが同期された状態を保持する
* ノードの稼働状況をモニタし、到達不可能になったノードから Pod を追い出す

#### ReplicaSet, the DaemonSet, and the Job controllers

* Replication Manager とほぼ同じ動作。それぞれ担当するリソースを watch し、必要に応じて Pod リソースを作成する (Pod を直接起動しない)

#### Deployment controller

* Deployment リソースを watch し、変更されるたびに新しいバージョンのロールアウトを行う -->

### 11.1.7. What the Kubelet does

* ![11fig01](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig01.jpg)

* Kubelet はノードに関する様々なことを担当するコンポーネント。
  * Node リソースを作成してノードを登録する
  * API サーバーの Pod リソースの変更を監視して、コンテナを起動する
  * コンテナを監視して、ステータスやイベント、リソース消費を API サーバーにレポートする
  * liveness プローブやコンテナの再起動
  * Pod の削除を検知してコンテナを停止する

* Kubelet は基本的に API サーバーと通信して Pod のマニフェストを取得するが、ノードのローカルにあるマニフェストを使って Pod を作成することもできる
* この機能はコントロールプレーンのコンポーネントをコンテナとして実行するときに使用される
* システムのネイティブコンポーネントだけでなく、カスタムシステムコンテナをこの方法で実行することもできるが、その場合は DaemonSet を使うべき。

* ![11fig08_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig08_alt.jpg)

### 11.1.8. The role of the Kubernetes Service Proxy

* ![11fig01](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig01.jpg)

* ノードでは kube-proxy も実行されている
* このコンポーネントの目的は、Kubernetes API を介して定義したサービスの IP とポートで、バックエンドの Pod に接続できるようにすること。
* サービス配下の Pod が複数ある場合には、ロードバランシングも行う。

* 初期の kube-proxy はユーザー空間で動作するプロキシとして実装されており、サービス IP への接続を受け付けられるように iptables のルールを変更してサーバープロセスに接続をリダイレクトし、そこから Pod にプロキシしていた (ユーザー空間プロキシモード)

* ![11fig09_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig09_alt.jpg)

* 現在はプロキシサーバーなしで、iptables ルールでバックエンドの Pod に接続をリダイレクトしている (iptables プロキシモード)

* ![11fig10_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig10_alt.jpg)

* 主要な違いは、パケットがユーザー空間で動作するプロキシサーバーのプロセスを通るか、カーネルのみで処理されるのかという点で、パフォーマンスに差異がある
* また、プロキシサーバーを経由する場合はラウンドロビンで接続先の Pod が選ばれるが、iptables プロキシモードの場合はランダムに Pod が選ばれる (Pod に接続が均等に分散されない可能性がある)

### 11.1.9. Introducing Kubernetes add-ons

* Kubernetes サービスの DNS ルックアップ、単一の External IP アドレスでの複数サービスの公開、Kubernetes ダッシュボードなどはコアコンポーネントではなくアドオンとして実現されている
* これらは Pod としてデプロイされている
* Deployment や ReplicationController リソースでデプロイされているものもあれば、DaemonSet でデプロイされているものもある

#### DNS

* クラスタ内の全 Pod は、デフォルトでクラスタの内部 DNS サーバを使用するように設定されている
* DNS サーバーも Pod としてデプロイされており、kube-dns サービスとして expose されている
* kube-dns の IP アドレスがクラスタにデプロイされている全てのコンテナの /etc/resolv.conf の nameserver に指定されるようになっている
* DNS サーバー Pod は Service や Endpoints リソースの変更を watch して変更があるたびに DNS レコードを更新するので、ほぼほぼ最新の DNS の情報を参照可能

```:bash
$ kubectl get services --namespace kube-system
NAME       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   6d

$ kubectl exec nginx-64f497f8fd-zvqwn cat /etc/resolv.conf
nameserver 10.100.0.10
search default.svc.cluster.local svc.cluster.local cluster.local ap-northeast-1.compute.internal
options ndots:5
```

#### Ingress

* Ingress コントローラはリバースプロキシサーバ (たとえばNginxなど) を実行し、クラスタで定義されている Ingress、Service、Endpoints の各リソースを watch してプロキシサーバーの設定を最新にする

## 11.2. HOW CONTROLLERS COOPERATE

Kubernetes がどのような動作をしているのかの理解を深めるため、Deployment リソースを作成して Pod が起動される時に何が行われるのかを見てみる。

### 11.2.1. Understanding which components are involved

* リソース作成前の状態。コントローラ、スケジューラ、Kubelet はそれぞれ API サーバーを watch している。

* ![11fig11_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig11_alt.jpg)

### 11.2.2. The chain of events

* kubectl で Deployment マニフェストを作成。API サーバーはマニフェストを検証して etcd に保存し、Deployment コントローラに通知。そこからリソース作成と通知が連鎖的に発生し、最終的にコンテナが起動する。

* ![11fig12_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig12_alt.jpg)

### 11.2.3. Observing cluster events

* コントロールプレーンのコンポーネントも Kubelet も、アクションを実行した際に API サーバーにイベントを発行する
* この時 Event リソースが作成されるので、`kubectl get events` のように他の Kubenetes リソースと同じように Event リソースを取得可能。

## 11.3. UNDERSTANDING WHAT A RUNNING POD IS

* Pod を起動すると、マニフェストで指定されたコンテナの他に /pause を実行するコンテナが立ち上がる
* pause コンテナは Pod 内のその他のコンテナがネットワークや Linux 名前空間を共有するためのコンテナ (インフラストラクチャコンテナ)。

* ![11fig13_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig13_alt.jpg)

* アプリケーションコンテナは再起動される可能性があるが、その際は以前と同じ Linux 名前空間に所属する必要がある => これを実現するためのインフラストラクチャコンテナ
* インフラストラクチャコンテナのライフサイクルは Pod に関連づけられており、Pod がスケジュールされてから削除されるまで実行され続ける
* 停止した場合は Kubelet がその Pod の全てのコンテナを再起動する

## 11.4. INTER-POD NETWORKING

* 各 Pod はそれぞれ固有の IP アドレスを取得してフラットなネットワークを介して Pod 間通信ができる
* ネットワークは Kubernetes 自体ではなく、ネットワーク管理者や Container Network Interface (CNI) によって設定される

### 11.4.1. What the network must be like

* Kubernetes では特定のネットワークテクノロジを使わなければならないという制限はないが、どのノードで Pod が動いているかに関わらず、Pod 同士が互いに通信できる必要がある。
* Pod から Pod に送信されたパケットは、送信元アドレスと宛先アドレスの両方を変更せずに相手側の Pod に到達する必要がある (NAT すべきではない)
* Pod 内で実行されているアプリケーションのネットワークが、同じネットワークスイッチに接続されているマシン上で実行されている場合と同じように動作させるために、これが重要。
* Pod・ノード間の通信も NAT-less でなければならない。

* ![11fig14_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig14_alt.jpg)

### 11.4.2. Diving deeper into how networking works

* Pod の IP アドレスやネットワーク名前空間はインフラストラクチャコンテナ (pause コンテナ) によって設定・保持されており、Pod 内のコンテナはそのネットワーク名前空間を使う。

* ![11fig15](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig15.jpg)

* インフラストラクチャコンテナが開始される前に、コンテナ用の仮想 NIC のペア (veth ペア) が作成される
* ペアの片方はホストのネットワーク名前空間にあり、ノードで ifconfig すると vethXXX のような形で見える
* もう片方はコンテナのネットワーク名前空間に移動され、eth0 にリネームされる
* ホスト側のネットワーク名前空間にある仮想 NIC はコンテナランタイムが使用するネットワークブリッジに接続されている。
* コンテナの eth0 インターフェイスには、ブリッジのアドレス範囲から IP アドレスが割り当てられている
* Pod 内のコンテナから eth0 にパケットを送信すると、eth0 -> vethXX -> ブリッジという流れで送信される (ブリッジに接続されちえるどの NIC でも受信することができる)
* 同一ノード上のコンテナはこれで互いに通信できるが、異なるノードのコンテナ間で通信を行えるようにするにはノード上のブリッジ同士を接続する必要がある
* ブリッジの接続にはオーバーレイネットワーク、アンダーレイネットワーク、または通常のレイヤ 3 ルーティングで実現可能

* ![11fig16_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig16_alt.jpg)

* 上の図にある通り、二つのノード間の Pod を通常のレイヤ 3 ルーティングで通信可能にするには、ノードの物理 NIC もブリッジに接続し、ノード A のルーティングテーブルにて 10.1.2.0/24 宛てのすべてのパケットがノード B にルーティングされるように設定する必要がある (逆も同様)。
* ノードが同じネットワークスイッチに接続されていて、その間にルータが存在しない場合には機能するが、そうでなければ Pod のプライベート IP を参照するのルータがパケットを落としてしまう。
* ルータの設定で対応することも可能だが、ノードが増えるにつれて設定ミスなどが発生しやすくなってしまう
* そのため、実際の基盤となるネットワークトポロジに関係なく、ノードが同じネットワークスイッチに接続されているように見えるソフトウェア定義ネットワーク (SDN) を使用する方が簡単

### 11.4.3. Introducing the Container Network Interface

**CNI について調べる**

## 11.5. HOW SERVICES ARE IMPLEMENTED

### 11.5.1. Introducing the kube-proxy

* サービスは固定 IP とポートを取得するためのもの
* サービスに関連するものはすべて、各ノードで実行されている kube-proxy プロセスによって処理される
* この IP アドレスは仮想的なものであり、どの NIC にも割り当てられていないので、Pod からパケットが送信される際に送信元や宛先としてこの IP が見えることはない
* サービスは IP とポートのペアから構成されるものであり、サービスの IP だけで何かを表すものではない (この IP に対して ping もできない)

### 11.5.2. How kube-proxy uses iptables

* API サーバーでサービスが作成されると仮想 IP がすぐに割り当てられ、その後ノードで稼働している kube-proxy agent にサービスが作成されたことが通知される
* それぞれの kube-proxy は、サービスの IP/ポートペア宛のパケットをインターセプトして、宛先アドレスが書き換えられるように iptables ルールを変更する
* これにより、パケットはサービス配下の Pod の一つにリダイレクトされるようになる
* kube-proxy は Endpoints オブジェクトの変更も監視している
* kube-proxy によって、クライアントがサービスを介して Pod に接続される流れを示したのか下図。

* ![11fig17_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig17_alt.jpg)

## 11.6. RUNNING HIGHLY AVAILABLE CLUSTERS

### 11.6.1. Making your apps highly available

* Kubernetes では、様々なコントローラによって、ノードに障害が発生しても指定された数のアプリが実行され続けるに対処される
* アプリの可用性を高めるには、Deploymentリソースを介して実行し、適切な数のレプリカを構成するだけ。後のことは Kubernetes が面倒を見てくれる
* 水平スケールが可能でないアプリでこの恩恵を受けるためには、レプリカ数 1 で Deployment を作成する必要があるが、この場合関連するすべてのコントローラがノード障害に気づき、Pod レプリカを新たに作成してコンテナを起動する、という処理を行う必要があるため、短時間のダウンタイムが発生する
* ダウンタイムを回避するには、アクティブ・スタンバイ構成にして、リーダー選択メカニズムが必要になる
* リーダー選出をアプリ自体に組み込む必要はなく、サイドカーコンテナを使って実現可能。https://github.com/kubernetes/contrib/tree/master/election に例がある。
* このように Kubernetes が面倒を見てくれるので、アプリ自体の可用性を高めることは容易。では Kubernetes 自体はどうか？

### 11.6.2. Making Kubernetes Control Plane components highly available

* Kubernetes 自体の可用性を高めるには、マスターノードを複数構成にする必要がある。

* ![11fig18_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig18_alt.jpg)

* etcd は分散システムとして設計されているので、複数の etcd インスタンスを実行可能
* API サーバーはステートレスであり、これも水平スケール可能。一般的には etcd と同じインスタンスで API サーバーも起動し、ローカルの etcd に接続するようにする
* クライアント (kubectl だけでなくコントローラマネージャやスケジューラ、Kubelet も) はロードバランサ経由で API サーバーにアクセスするようにする
* 複数のコントローラマネージャやスケジューラを同時にアクティブにすると、それらが同時にクラスタの状態変化を検知して同じアクションを行い、競合が発生する可能性があるので望ましくない
* マスターノードを冗長化するときは、これらのコンポーネントはアクティブ・スタンバイ構成にする
* コントローラマネージャやスケジューラは API サーバーや etcd と同じインスタンスで実行することも、別のインスタンスで実行することも可能。同じインスタンスで実行している場合はローカルの API サーバーに直接アクセスできるが、別で実行している場合はロードバランサ経由でアクセスする

* ![11fig19_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig19_alt.jpg)

* リーダー選出は API サーバーにリソースを作成することで行なっている (コンポーネント間の通信は不要)
* 例えばスケジューラの場合
  * kube-system 名前空間 に kube-scheduler という名前の Endpoints リソースを作成
  * このリソースの control-plane.alpha.kubernetes.io/leader アノテーションの holderIdentity フィールドに自分の名前 (スケジューラが稼働しているノードの名前？) を設定できたスケジューラがリーダーとなる
  * リーダーは定期的に (デフォルトでは2秒ごとに) リソースを更新する (アノテーションの renewTime が更新されている)
  * この更新が行われなくなったことを他のスケジューラが検知すると、holderIdentity を自分の名前に書き換えてリーダーになろうとする
* `kubectl get endpoints kube-scheduler -n kube-system -yaml —watch` すると、確かに 2 秒ごとに更新されている。

```
$ kubectl get endpoints kube-scheduler -n kube-system -o yaml --watch | grep renewTime
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"ip-10-0-36-254.ap-northeast-1.compute.internal_f4e174c2-572e-11e9-ba39-067e565f52c2","leaseDurationSeconds":15,"acquireTime":"2019-04-04T23:19:52Z","renewTime":"2019-04-08T03:31:38Z","leaderTransitions":1}'
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"ip-10-0-36-254.ap-northeast-1.compute.internal_f4e174c2-572e-11e9-ba39-067e565f52c2","leaseDurationSeconds":15,"acquireTime":"2019-04-04T23:19:52Z","renewTime":"2019-04-08T03:31:40Z","leaderTransitions":1}'
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"ip-10-0-36-254.ap-northeast-1.compute.internal_f4e174c2-572e-11e9-ba39-067e565f52c2","leaseDurationSeconds":15,"acquireTime":"2019-04-04T23:19:52Z","renewTime":"2019-04-08T03:31:42Z","leaderTransitions":1}'
...
```

## 11.7. SUMMARY

* Kubernetes クラスタを構成するコンポーネントとそれらの役割
* API サーバーやスケジューラ、各種コントローラ、Kubelet がどのように動作するのか
* インフラストラクチャ Pod がどのように Pod 内のコンテナ間のバインドを行うか
* Pod が同一ノード内やノード間でどのように通信しているのか
* kube-proxy が iptables を使ってどのように Pod のロードバランシングを行なっているのか
* クラスタの HA を実現するためにコントロールプレーンのコンポーネントが複数のインスタンスでどのように動作しているのか
