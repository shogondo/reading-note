# Chapter 11. Understanding Kubernetes internals

## 11.1. UNDERSTANDING THE ARCHITECTURE

おさらい。Kubernetes クラスタは次のようなコンポーネントからなる。

* コントロールプレーン
  * etcd
  * API サーバー
  * スケジューラ
  * コントローラマネージャ
* ワーカーノード
  * Kubelet
  * Kubernetes サービスプロキシ (kube-proxy)
  * コンテナランタイム (Docker, rkt など)
* アドオン
  * Kubernetes DNS サーバー
  * ダッシュボード
  * Ingress コントローラ
  * Heapster (14 章で取り上げる)
  * CNI ネットワークプラグイン (この章で後ほど取り上げる)

### 11.1.1. The distributed nature of Kubernetes components

各コンポーネントはそれぞれ独立したプロセスとして実行される。コンポーネントとそれらの依存関係の概要が次の図。

* ![11fig01](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig01.jpg)

* Kubernetes のシステムコンポーネントは直接やりとりせず、API サーバーのみを使ってコミュニケーションする。
* API サーバーは etcd と直接通信可能な唯一のコンポーネント。
* その他のコンポーネントは API サーバーを介してクラスタの状態を更新する。
* コントロールプレーンのコンポーネントは複数のホストに分割して稼働させ、高可用性を実現することができる。
* etcd と API サーバーは複数のホストでパラレルに実行可能。
* スケジューラやコントローラマネージャはアクティブ・スタンバイ構成になる。

**EKS の場合の構成は？**

* Kubelet は常に通常のシステムコンポーネントとして稼働させ必要があるが、コントロールプレーンのコンポーネントと kube-proxy は Pod として稼働させることも可能。
* コントロールプレーンのコンポーネントを Pod 化したい場合は、Kubelet もマスターにデプロイする必要がある。

**EKS で可能？**


### 11.1.2. How Kubernetes uses etcd

* Kubernetes はクラスタの状態やメタデータを永続化するために etcd を使用している
* etcd に唯一直接アクセス可能なのが API サーバーで、その他のコンポーネントは API サーバー経由で間接的にアクセスする
* そのため、一貫した楽観的排他制御やバリデーション、永続化方法の抽象化が可能

**All Kubernetes resources include a metadata.resourceVersion field, which clients need to pass back to the API server when updating an object** これは本当か？

* この書籍執筆時点では、Kubernetes では etcd v2 と v3 が利用可能だが、パフォーマンス改善が施されている v3 の利用が推奨される

**EKS の etcd のバージョンは？**
**EKS の etcd に直接接続可能か？**

* Kubernetes は /registry というキープレフィックス配下に全てのデータを格納する。

```:bash
$ etcdctl ls /registry
/registry/configmaps
/registry/daemonsets
/registry/deployments
/registry/events
/registry/namespaces
/registry/pods
...
```

* Pod の場合は、/registry/pods/<名前空間>/<Pod 名> のようなキーに情報が格納される。
* 格納されるのは Pod 定義の JSON そのもの。

```
$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga
{"kind":"Pod","apiVersion":"v1","metadata":{"name":"kubia-159041347-wt6ga",
"generateName":"kubia-159041347-","namespace":"default","selfLink":...
```

* etcd をマルチインスタンス構成にすることで可用性を高めることができる
* 一方、一貫した状態を保つためにマルチインスタンス間でコンセンサスが必要
* etcd は RAFT コンセンサスアルゴリズムを使う
* RAFT では、状態遷移をさせるには過半数のノードの同意が必要
* ノード群が二つのグループにネットワーク的に分断されても、それぞれのグループの状態が分岐して別々に遷移してしまうことはない
* 状態遷移可能なのは過半数のノードが参加しているグループのみ
* ネットワーク分断が解消された際に、過半数でなかったグループのノードがもう一方のグループの状態に同期される

**RAFT について調べる**

* ![11fig02_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig02_alt.jpg)

* etcd は一般的に奇数のインスタンスにデプロイされる
* 例えば etcd が 2 インスタンス構成の場合、片方のインスタンスで問題が発生した時点で状態遷移ができなくなる (過半数の同意、が得られなくなるので)
* 通常、大きなクラスタでは etcd は 5 から 7 ノードで十分 (2 or 3 ノードの問題に対応できる)

**EKS は何ノード構成なのか？**

### 11.1.3. What the API server does

* API サーバーは他のコンポーネントやクライアント (kubectl など) から利用される中心的なコンポーネントで、これらに対して RESTFul API を提供する
* 状態を etcd に保存し、バリデーションや楽観的排他制御を担当している
* 例えば kubectl でリソース作成を行なった場合、HTTP POST でマニフェストを API サーバーに送信され、下図のような流れで処理が行われる。

* ![11fig03_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig03_alt.jpg)

* まず、Authentication プラグインによってリクエストの認証が行われる
* APIサーバーは、誰がリクエストを送信したのかを判断するまで、これらのプラグインを順番に呼び出す
* 次に Authorization プラグインによって、認証されたユーザーがリクエストされたリソースに対するアクションを行う権限があるのか確認する
* リクエストがリソースの作成、更新、削除なら次に Admission Control プラグインが実行される
* Admission Control プラグインによって、リソース仕様から欠けているフィールドをデフォルト値に初期化する、リクエストを拒否する、といったことが発生する
* リクエストが読み取りしか行わない場合は Admission Control プラグインは実行されない
* Admission Control プラグインの例
  * AlwaysPullImages: Pod の imagePullPolicy を Always に上書きする (常にイメージを pull することを強制する)
  * ServiceAccount: Pod にデフォルトサービスアカウントを適用する
  * NamespaceLifecycle: 削除中や存在しないネームスペースに Pod が作成されるのを防ぐ
  * ResourceQuota: 特定のネームスペース内の Pod が、ネームスペースに割り当てられているのと同じだけの CPU とメモリを使用するようにする。
* その他の Admission Control プラグインについては https://kubernetes.io/docs/admin/admission-controllers/ を参照
* リクエストが全ての Admission Control を通過したら、API サーバーはバリデーションを行い、データを etcd に格納する


**EKS の Auth について**
**Admission Control について**

### 11.1.4. Understanding how the API server notifies clients of resource changes

* API サーバーは上記以外のことは行わない。
* つまり、Pod を作ったりするのは API サーバーの役割ではない。
* API サーバーは他のコンポーネントに対してリソースの変更を検知できる機能を提供する
* コンポーネントはクラスタメタデータの変更を検知して、何か処理を行うことができる

* ![11fig04_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig04_alt.jpg)

* kubectl でも変更を subscribe することが可能 (--watch オプション)
* 例えば `kubectl get pods --watch` しながら Pod を作成した場合、Pod のステータス等に変更があるたびにその状態がコンソールに出力されるような動作になる。

### 11.1.5. Understanding the Scheduler

* スケジューラは、どのノードで Pod を稼働させるかを選択するコンポーネント。
* スケジューラが実行するのは、API サーバーを介してポッド定義を更新することだけ (選択したノードにポッドを実行するように直接指示するわけではない)。
* API サーバーから変更を watch している Kubelet に対してその変更の通知が送られ、Pod が起動されるという流れになる。

* ノードの選択は大きく分けると二つのパートからなる
* Pod をスケジュールすることが可能なノードのみをフィルタする
* その中から最も適切なノードを優先させる (同じプライオリティのノードが複数あればラウンドロビンで選択)

* ![11fig05_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig05_alt.jpg)

* 以下のような観点でスケジューラは Pod をスケジュールすることが可能なノードのみをフィルタする
* Pod のハードウェア要求を満たすノードか
* ノードのリソースが足りているか
* Pod のメタデータで by name で指定されたノードか
* ノードセレクタにマッチするラベルを持っているか
* Pod がバインドを要求しているポートがすでに使われているか
* Pod が要求しているボリュームを使えるか (ノードがマウントしていて、他の Pod が使っていないか)
* Taints/Tolerations や Affinity のルール

* 各ノードに対して上のチェックを行い、Pod をスケジュール可能なノードを洗い出す

* スケジュール可能なノードが複数ある場合にどのノードを選択すべきかは、複数のファクターが絡むなかなか難しい問題
  * ノードが二つあり、一方のノードで Pod が動いている状態でもう一つ起動する場合は、もう一方のノードで起動してほしい
  * だが、クラウドインフラでノードを起動しているのなら、一方のノードに Pod をまとめてもう一つのノードを停止してコストを節約する方がベターかもしれない。
  * でも複数の Pod のレプリカを起動している状況なら、ノード障害時の影響を抑えるためできるだけ複数ノードに分散してほしい
* そのため、スケジューラはニーズに合わせて設定したり、独自実装に置き換えたりできる
* 複数のスケジューラを使うことも可能。
* Pod にスケジューラ名を指定して、どのスケジューラでスケジュールさせるのか指定できる

### 11.1.6. Introducing the controllers running in the Controller Manager

* API サーバーはリソースの情報を etcd に格納してクライアントに通知するだけ、スケジューラは Pod をノードにアサインするだけ。
* クラスタを期待された状態 (etcd に格納されているリソースの状態) にするためのコンポーネントが必要 => これがコントローラマネージャ内で稼働しているコントローラ
* 現在は単一のコントローラマネージャプロセスが各コントローラの処理を実行しているが、最終的にはコントローラごとに別々のプロセスとし、カスタム実装で置き換えられるようになる

**現状どうなのか**

* このようなコントローラがある
* Replication Manager (a controller for ReplicationController resources)
* ReplicaSet, DaemonSet, and Job controllers
* Deployment controller
* StatefulSet controller
* Node controller
* Service controller
* Endpoints controller
* Namespace controller
* PersistentVolume controller

* コントローラは API サーバーの変更を watch し、リソースの作成や更新を検知してクラスタをその状態にするための処理を行う
* 基本的には watch を使っているが、全てのイベントが拾えることが保証されていないので、何か処理漏れがないかを確認するため定期的なポーリングも合わせて行なっている
* コントローラ同士はお互いに直接通信しない (お互いの存在すら知らない)
* コントローラのコード https://github.com/kubernetes/kubernetes/tree/master/pkg/controller

**ここ省略**

### 11.1.7. What the Kubelet does

* Kubelet はノードに関する様々なことを担当するコンポーネント。
  * Node リソースを作成してノードを登録する
  * API サーバーの Pod リソースの変更を監視して、コンテナを起動する
  * コンテナを監視して、ステータスやイベント、リソース消費を API サーバーにレポートする
  * liveness プローブやコンテナの再起動
  * Pod の削除を検知してコンテナを停止する

* Kubelet は基本的に API サーバーと通信して Pod のマニフェストを取得するが、ノードのローカルにあるマニフェストを使って Pod を作成することもできる
* この機能はコントロールプレーンのコンポーネントをコンテナとして実行するときに使用される
* システムのネイティブコンポーネントだけでなく、カスタムシステムコンテナをこの方法で実行することもできるが、その場合は DaemonSet を使うべき。

* ![11fig08_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig08_alt.jpg)

### 11.1.8. The role of the Kubernetes Service Proxy

* ノードでは kube-proxy も実行されている
* このコンポーネントの目的は、Kubernetes API を介して定義したサービスの IP とポートで、バックエンドの Pod に接続できるようにすること。
* サービス配下の Pod が複数ある場合には、ロードバランシングも行う。

* 初期の kube-proxy はユーザー空間で動作するプロキシとして実装されており、サービス IP への接続を受け付けられるように iptables のルールを変更してサーバープロセスに接続をリダイレクトし、そこから Pod にプロキシしていた (ユーザー空間プロキシモード)

* ![11fig09_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig09_alt.jpg)

* 現在はプロキシサーバーなしで、iptables ルールでバックエンドの Pod に接続をリダイレクトしている (iptables プロキシモード)

* ![11fig10_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig10_alt.jpg)

* 主要な違いは、パケットがユーザー空間で動作するプロキシサーバーのプロセスを通るか、カーネルのみで処理されるのかという点で、パフォーマンスに差異がある
* また、プロキシサーバーを経由する場合はラウンドロビンで接続先の Pod が選ばれるが、iptables プロキシモードの場合はランダムに Pod が選ばれる (Pod に接続が均等に分散されない可能性がある)

### 11.1.9. Introducing Kubernetes add-ons

* Kubernetes サービスの DNS ルックアップ、単一の External IP アドレスでの複数サービスの公開、Kubernetes ダッシュボードはコアコンポーネントではなくアドオンとして実現されている
* これらは Pod としてデプロイされている
* Deployment や ReplicationController リソースでデプロイされているものもあれば、DaemonSet でデプロイされているものもある

* クラスタ内の全 Pod は、デフォルトでクラスタの内部 DNS サーバを使用するように設定されている
* DNS server Pod は、kube-dns サービスを通じて公開される
* 他の Pod と同様に、Pod をクラスター内で移動させることができる
* **DNSサーバーポッドは、kube-dnsサービスを通じて公開され、他のポッドと同様に、ポッドをクラスター内で移動させることができます。 サービスのIPアドレスは、クラスタにデプロイされているすべてのコンテナ内の/etc/resolv.confファイルでネームサーバーとして指定されています。 kube-dnsポッドは、APIサーバーの監視メカニズムを使用してサービスとエンドポイントへの変更を監視し、変更があるたびにDNSレコードを更新するので、クライアントは常に（かなり）最新のDNS情報を取得できます。 ServiceまたはEndpointsリソースの更新とDNSポッドが監視通知を受信するまでの間にDNSレコードが無効になる可能性があるので、私は正直に言います。**

**DNS server Pod と kube-dns pod は違うもの？**
**coredns との違い？**

* **DNSアドオンとは異なり、Ingressコントローラにはいくつか異なる実装がありますが、それらのほとんどは同じように機能します。 Ingressコントローラはリバースプロキシサーバ（たとえばNginxなど）を実行し、クラスタで定義されているIngress、Service、およびEndpointsの各リソースに従って設定を維持します。 そのため、コントローラはこれらのリソースを監視し（監視メカニズムを通じて）、リソースが変更されるたびにプロキシサーバーの設定を変更する必要があります。**
* **Ingressリソースの定義はサービスを指していますが、IngressコントローラはサービスIPを経由するのではなく、サービスのポッドにトラフィックを直接転送します。 これは、外部クライアントがIngressコントローラを介して接続するときのクライアントIPの保存に影響するため、特定のユースケースではサービスよりも優先されます。**

* **DNSサーバーとIngressコントローラーアドオンの両方がController Managerで実行されているコントローラーとどのように似ているかを確認しました。ただし、APIサーバーを介してリソースを監視および変更するだけでなく、クライアント接続も受け入れます。**
* **他のアドオンも同様です。 全員がクラスタの状態を観察し、それが変化したときに必要なアクションを実行する必要があります。 この章と残りの章では、他にもいくつかのアドオンを紹介します。**

### 11.1.10. Bringing it all together

NO need

## 11.2. HOW CONTROLLERS COOPERATE

Kubernetes がどのような動作をしているのかの理解を深めるため、Deployment リソースを作成して Pod が起動される時に何が行われるのかを見てみる。

### 11.2.1. Understanding which components are involved

リソース作成前の状態。コントローラ、スケジューラ、Kubelet はそれぞれ API サーバーを watch している。

* ![11fig11_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig11_alt.jpg)

### 11.2.2. The chain of events

kubectl で Deployment マニフェストを作成。API サーバーはマニフェストを検証して etcd に保存し、Deployment コントローラに通知。そこからリソース作成と通知が連鎖的に発生し、最終的にコンテナが起動する。

* ![11fig12_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig12_alt.jpg)

### 11.2.3. Observing cluster events

コントロールプレーンのコンポーネントも Kubelet も、アクションを実行した際に API サーバーにイベントを発行する
この時 Event リソースが作成されるので、`kubectl get events` のように他の Kubenetes リソースと同じように Event リソースを取得可能。

## 11.3. UNDERSTANDING WHAT A RUNNING POD IS

* ![11fig13_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig13_alt.jpg)

## 11.4. INTER-POD NETWORKING

### 11.4.1. What the network must be like

* ![11fig14_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig14_alt.jpg)

### 11.4.2. Diving deeper into how networking works

* ![11fig15](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig15.jpg)
* ![11fig16_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig16_alt.jpg)

### 11.4.3. Introducing the Container Network Interface

## 11.5. HOW SERVICES ARE IMPLEMENTED

### 11.5.1. Introducing the kube-proxy

### 11.5.2. How kube-proxy uses iptables

* ![11fig17_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig17_alt.jpg)

## 11.6. RUNNING HIGHLY AVAILABLE CLUSTERS

### 11.6.1. Making your apps highly available

### 11.6.2. Making Kubernetes Control Plane components highly available

* ![11fig18_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig18_alt.jpg)
* ![11fig19_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig19_alt.jpg)

## 11.7. SUMMARY

* Kubernetes クラスタを構成するコンポーネントとそれらの役割
* API サーバーやスケジューラ、各種コントローラ、Kubelet がどのように動作するのか
* How the infrastructure container binds together all the containers of a pod
* ポッドが同一ノード内やノード間でどのように通信しているのか
* kube-proxy が iptables を使ってどのように Pod のロードバランシングを行なっているのか
* クラスタの HA を実現するためにコントロールプレーンのコンポーネントが複数のインスタンスでどのように動作しているのか
