# Chapter 11. Understanding Kubernetes internals

## 11.1. UNDERSTANDING THE ARCHITECTURE

おさらい。Kubernetes クラスタは次のようなコンポーネントからなる。

* コントロールプレーン
  * etcd
  * API サーバー
  * スケジューラ
  * コントローラマネージャ
* ワーカーノード
  * Kubelet
  * Kubernetes サービスプロキシ (kube-proxy)
  * コンテナランタイム (Docker, rkt など)
* アドオン
  * Kubernetes DNS サーバー
  * ダッシュボード
  * Ingress コントローラ
  * Heapster (14 章で取り上げる)
  * CNI ネットワークプラグイン (この章で後ほど取り上げる)

### 11.1.1. The distributed nature of Kubernetes components

各コンポーネントはそれぞれ独立したプロセスとして実行される。コンポーネントとそれらの依存関係の概要が次の図。

* ![11fig01](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig01.jpg)

* Kubernetes のシステムコンポーネントは直接やりとりせず、API サーバーのみを使ってコミュニケーションする。
* API サーバーは etcd と直接通信可能な唯一のコンポーネント。
* その他のコンポーネントは API サーバーを介してクラスタの状態を更新する。
* コントロールプレーンのコンポーネントは複数のホストに分割して稼働させ、高可用性を実現することができる。
* etcd と API サーバーは複数のホストでパラレルに実行可能。
* スケジューラやコントローラマネージャはアクティブ・スタンバイ構成になる。

**EKS の場合の構成は？**

* Kubelet は常に通常のシステムコンポーネントとして稼働させ必要があるが、コントロールプレーンのコンポーネントと kube-proxy は Pod として稼働させることも可能。
* コントロールプレーンのコンポーネントを Pod 化したい場合は、Kubelet もマスターにデプロイする必要がある。

**EKS で可能？**


### 11.1.2. How Kubernetes uses etcd

* Kubernetes はクラスタの状態やメタデータを永続化するために etcd を使用している
* etcd に唯一直接アクセス可能なのが API サーバーで、その他のコンポーネントは API サーバー経由で間接的にアクセスする
* そのため、一貫した楽観的排他制御やバリデーション、永続化方法の抽象化が可能

**All Kubernetes resources include a metadata.resourceVersion field, which clients need to pass back to the API server when updating an object** これは本当か？

* この書籍執筆時点では、Kubernetes では etcd v2 と v3 が利用可能だが、パフォーマンス改善が施されている v3 の利用が推奨される

**EKS の etcd のバージョンは？**
**EKS の etcd に直接接続可能か？**

* Kubernetes は /registry というキープレフィックス配下に全てのデータを格納する。

```:bash
$ etcdctl ls /registry
/registry/configmaps
/registry/daemonsets
/registry/deployments
/registry/events
/registry/namespaces
/registry/pods
...
```

* Pod の場合は、/registry/pods/<名前空間>/<Pod 名> のようなキーに情報が格納される。
* 格納されるのは Pod 定義の JSON そのもの。

```
$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga
{"kind":"Pod","apiVersion":"v1","metadata":{"name":"kubia-159041347-wt6ga",
"generateName":"kubia-159041347-","namespace":"default","selfLink":...
```

* etcd をマルチインスタンス構成にすることで可用性を高めることができる
* 一方、一貫した状態を保つためにマルチインスタンス間でコンセンサスが必要
* etcd は RAFT コンセンサスアルゴリズムを使う
* RAFT では、状態遷移をさせるには過半数のノードの同意が必要
* ノード群が二つのグループにネットワーク的に分断されても、それぞれのグループの状態が分岐して別々に遷移してしまうことはない
* 状態遷移可能なのは過半数のノードが参加しているグループのみ
* ネットワーク分断が解消された際に、過半数でなかったグループのノードがもう一方のグループの状態に同期される

**RAFT について調べる**

* ![11fig02_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig02_alt.jpg)

* etcd は一般的に奇数のインスタンスにデプロイされる
* 例えば etcd が 2 インスタンス構成の場合、片方のインスタンスで問題が発生した時点で状態遷移ができなくなる (過半数の同意、が得られなくなるので)
* 通常、大きなクラスタでは etcd は 5 から 7 ノードで十分 (2 or 3 ノードの問題に対応できる)

**EKS は何ノード構成なのか？**

### 11.1.3. What the API server does

* API サーバーは他のコンポーネントやクライアント (kubectl など) から利用される中心的なコンポーネントで、これらに対して RESTFul API を提供する
* 状態を etcd に保存し、バリデーションや楽観的排他制御を担当している
* 例えば kubectl でリソース作成を行なった場合、HTTP POST でマニフェストを API サーバーに送信され、下図のような流れで処理が行われる。

* ![11fig03_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig03_alt.jpg)

* まず、Authentication プラグインによってリクエストの認証が行われる
* APIサーバーは、誰がリクエストを送信したのかを判断するまで、これらのプラグインを順番に呼び出す
* 次に Authorization プラグインによって、認証されたユーザーがリクエストされたリソースに対するアクションを行う権限があるのか確認する
* リクエストがリソースの作成、更新、削除なら次に Admission Control プラグインが実行される
* Admission Control プラグインによって、リソース仕様から欠けているフィールドをデフォルト値に初期化する、リクエストを拒否する、といったことが発生する
* リクエストが読み取りしか行わない場合は Admission Control プラグインは実行されない
* Admission Control プラグインの例
  * AlwaysPullImages: Pod の imagePullPolicy を Always に上書きする (常にイメージを pull することを強制する)
  * ServiceAccount: Pod にデフォルトサービスアカウントを適用する
  * NamespaceLifecycle: 削除中や存在しないネームスペースに Pod が作成されるのを防ぐ
  * ResourceQuota: 特定のネームスペース内の Pod が、ネームスペースに割り当てられているのと同じだけの CPU とメモリを使用するようにする。
* その他の Admission Control プラグインについては https://kubernetes.io/docs/admin/admission-controllers/ を参照
* リクエストが全ての Admission Control を通過したら、API サーバーはバリデーションを行い、データを etcd に格納する


**EKS の Auth について**
**Admission Control について**

### 11.1.4. Understanding how the API server notifies clients of resource changes

* API サーバーは上記以外のことは行わない。
* つまり、Pod を作ったりするのは API サーバーの役割ではない。
* API サーバーは他のコンポーネントに対してリソースの変更を検知できる機能を提供する
* コンポーネントはクラスタメタデータの変更を検知して、何か処理を行うことができる

* ![11fig04_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig04_alt.jpg)

* kubectl でも変更を subscribe することが可能 (--watch オプション)
* 例えば `kubectl get pods --watch` しながら Pod を作成した場合、Pod のステータス等に変更があるたびにその状態がコンソールに出力されるような動作になる。

### 11.1.5. Understanding the Scheduler

* スケジューラは、どのノードで Pod を稼働させるかを選択するコンポーネント。
* スケジューラが実行するのは、API サーバーを介してポッド定義を更新することだけ (選択したノードにポッドを実行するように直接指示するわけではない)。
* API サーバーから変更を watch している Kubelet に対してその変更の通知が送られ、Pod が起動されるという流れになる。

* ノードの選択は大きく分けると二つのパートからなる
* Pod をスケジュールすることが可能なノードのみをフィルタする
* その中から最も適切なノードを優先させる (同じプライオリティのノードが複数あればラウンドロビンで選択)

* ![11fig05_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig05_alt.jpg)

* 以下のような観点でスケジューラは Pod をスケジュールすることが可能なノードのみをフィルタする
* Pod のハードウェア要求を満たすノードか
* ノードのリソースが足りているか
* Pod のメタデータで by name で指定されたノードか
* ノードセレクタにマッチするラベルを持っているか
* Pod がバインドを要求しているポートがすでに使われているか
* Pod が要求しているボリュームを使えるか (ノードがマウントしていて、他の Pod が使っていないか)
* Taints/Tolerations や Affinity のルール

* 各ノードに対して上のチェックを行い、Pod をスケジュール可能なノードを洗い出す

* スケジュール可能なノードが複数ある場合にどのノードを選択すべきかは、複数のファクターが絡むなかなか難しい問題
  * ノードが二つあり、一方のノードで Pod が動いている状態でもう一つ起動する場合は、もう一方のノードで起動してほしい
  * だが、クラウドインフラでノードを起動しているのなら、一方のノードに Pod をまとめてもう一つのノードを停止してコストを節約する方がベターかもしれない。
  * でも複数の Pod のレプリカを起動している状況なら、ノード障害時の影響を抑えるためできるだけ複数ノードに分散してほしい
* そのため、スケジューラはニーズに合わせて設定したり、独自実装に置き換えたりできる
* 複数のスケジューラを使うことも可能。
* Pod にスケジューラ名を指定して、どのスケジューラでスケジュールさせるのか指定できる

### 11.1.6. Introducing the controllers running in the Controller Manager

* API サーバーはリソースの情報を etcd に格納してクライアントに通知するだけ、スケジューラは Pod をノードにアサインするだけ。
* クラスタを期待された状態 (etcd に格納されているリソースの状態) にするためのコンポーネントが必要 => これがコントローラマネージャ内で稼働しているコントローラ
* 現在は単一のコントローラマネージャプロセスが各コントローラの処理を実行しているが、最終的にはコントローラごとに別々のプロセスとし、カスタム実装で置き換えられるようになる

**現状どうなのか**

* このようなコントローラがある
* Replication Manager (a controller for ReplicationController resources)
* ReplicaSet, DaemonSet, and Job controllers
* Deployment controller
* StatefulSet controller
* Node controller
* Service controller
* Endpoints controller
* Namespace controller
* PersistentVolume controller

* コントローラは API サーバーの変更を watch し、リソースの作成や更新を検知してクラスタをその状態にするための処理を行う
* 基本的には watch を使っているが、全てのイベントが拾えることが保証されていないので、何か処理漏れがないかを確認するため定期的なポーリングも合わせて行なっている
* コントローラ同士はお互いに直接通信しない (お互いの存在すら知らない)
* コントローラのコード https://github.com/kubernetes/kubernetes/tree/master/pkg/controller

**ここ省略**

### 11.1.7. What the Kubelet does

* Kubelet はノードに関する様々なことを担当するコンポーネント。
  * Node リソースを作成してノードを登録する
  * API サーバーの Pod リソースの変更を監視して、コンテナを起動する
  * コンテナを監視して、ステータスやイベント、リソース消費を API サーバーにレポートする
  * liveness プローブやコンテナの再起動
  * Pod の削除を検知してコンテナを停止する

* Kubelet は基本的に API サーバーと通信して Pod のマニフェストを取得するが、ノードのローカルにあるマニフェストを使って Pod を作成することもできる
* この機能はコントロールプレーンのコンポーネントをコンテナとして実行するときに使用される
* システムのネイティブコンポーネントだけでなく、カスタムシステムコンテナをこの方法で実行することもできるが、その場合は DaemonSet を使うべき。

* ![11fig08_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig08_alt.jpg)

### 11.1.8. The role of the Kubernetes Service Proxy

* ノードでは kube-proxy も実行されている
* このコンポーネントの目的は、Kubernetes API を介して定義したサービスの IP とポートで、バックエンドの Pod に接続できるようにすること。
* サービス配下の Pod が複数ある場合には、ロードバランシングも行う。

* 初期の kube-proxy はユーザー空間で動作するプロキシとして実装されており、サービス IP への接続を受け付けられるように iptables のルールを変更してサーバープロセスに接続をリダイレクトし、そこから Pod にプロキシしていた (ユーザー空間プロキシモード)

* ![11fig09_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig09_alt.jpg)

* 現在はプロキシサーバーなしで、iptables ルールでバックエンドの Pod に接続をリダイレクトしている (iptables プロキシモード)

* ![11fig10_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig10_alt.jpg)

* 主要な違いは、パケットがユーザー空間で動作するプロキシサーバーのプロセスを通るか、カーネルのみで処理されるのかという点で、パフォーマンスに差異がある
* また、プロキシサーバーを経由する場合はラウンドロビンで接続先の Pod が選ばれるが、iptables プロキシモードの場合はランダムに Pod が選ばれる (Pod に接続が均等に分散されない可能性がある)

### 11.1.9. Introducing Kubernetes add-ons

* Kubernetes サービスの DNS ルックアップ、単一の External IP アドレスでの複数サービスの公開、Kubernetes ダッシュボードはコアコンポーネントではなくアドオンとして実現されている
* これらは Pod としてデプロイされている
* Deployment や ReplicationController リソースでデプロイされているものもあれば、DaemonSet でデプロイされているものもある

* クラスタ内の全 Pod は、デフォルトでクラスタの内部 DNS サーバを使用するように設定されている
* DNS server Pod は、kube-dns サービスを通じて公開される
* 他の Pod と同様に、Pod をクラスター内で移動させることができる
* **DNSサーバーポッドは、kube-dnsサービスを通じて公開され、他のポッドと同様に、ポッドをクラスター内で移動させることができます。 サービスのIPアドレスは、クラスタにデプロイされているすべてのコンテナ内の/etc/resolv.confファイルでネームサーバーとして指定されています。 kube-dnsポッドは、APIサーバーの監視メカニズムを使用してサービスとエンドポイントへの変更を監視し、変更があるたびにDNSレコードを更新するので、クライアントは常に（かなり）最新のDNS情報を取得できます。 ServiceまたはEndpointsリソースの更新とDNSポッドが監視通知を受信するまでの間にDNSレコードが無効になる可能性があるので、私は正直に言います。**

**DNS server Pod と kube-dns pod は違うもの？**
**coredns との違い？**

* **DNSアドオンとは異なり、Ingressコントローラにはいくつか異なる実装がありますが、それらのほとんどは同じように機能します。 Ingressコントローラはリバースプロキシサーバ（たとえばNginxなど）を実行し、クラスタで定義されているIngress、Service、およびEndpointsの各リソースに従って設定を維持します。 そのため、コントローラはこれらのリソースを監視し（監視メカニズムを通じて）、リソースが変更されるたびにプロキシサーバーの設定を変更する必要があります。**
* **Ingressリソースの定義はサービスを指していますが、IngressコントローラはサービスIPを経由するのではなく、サービスのポッドにトラフィックを直接転送します。 これは、外部クライアントがIngressコントローラを介して接続するときのクライアントIPの保存に影響するため、特定のユースケースではサービスよりも優先されます。**

* **DNSサーバーとIngressコントローラーアドオンの両方がController Managerで実行されているコントローラーとどのように似ているかを確認しました。ただし、APIサーバーを介してリソースを監視および変更するだけでなく、クライアント接続も受け入れます。**
* **他のアドオンも同様です。 全員がクラスタの状態を観察し、それが変化したときに必要なアクションを実行する必要があります。 この章と残りの章では、他にもいくつかのアドオンを紹介します。**

### 11.1.10. Bringing it all together

NO need

## 11.2. HOW CONTROLLERS COOPERATE

Kubernetes がどのような動作をしているのかの理解を深めるため、Deployment リソースを作成して Pod が起動される時に何が行われるのかを見てみる。

### 11.2.1. Understanding which components are involved

リソース作成前の状態。コントローラ、スケジューラ、Kubelet はそれぞれ API サーバーを watch している。

* ![11fig11_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig11_alt.jpg)

### 11.2.2. The chain of events

kubectl で Deployment マニフェストを作成。API サーバーはマニフェストを検証して etcd に保存し、Deployment コントローラに通知。そこからリソース作成と通知が連鎖的に発生し、最終的にコンテナが起動する。

* ![11fig12_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig12_alt.jpg)

### 11.2.3. Observing cluster events

コントロールプレーンのコンポーネントも Kubelet も、アクションを実行した際に API サーバーにイベントを発行する
この時 Event リソースが作成されるので、`kubectl get events` のように他の Kubenetes リソースと同じように Event リソースを取得可能。

## 11.3. UNDERSTANDING WHAT A RUNNING POD IS

* Pod を起動すると、マニフェストで指定されたコンテナの他に /pause を実行するコンテナが立ち上がる
* pause コンテナは Pod 内のその他のコンテナがネットワークや Linux 名前空間を共有するためのコンテナ (インフラストラクチャコンテナ)。

* **調査**

* ![11fig13_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig13_alt.jpg)

* アプリケーションコンテナは再起動される可能性があるが、その際は以前と同じ Linux 名前空間に所属する必要がある
* インフラストラクチャコンテナのライフサイクルは Pod に関連づけられており、Pod がスケジュールされてから削除されるまで実行され続ける
* 停止した場合は Kubelet がその Pod の全てのコンテナを再起動する

## 11.4. INTER-POD NETWORKING

* 各 Pod はそれぞれ固有の IP アドレスを取得してフラットなネットワークを介して Pod 間通信ができる
* ネットワークは Kubernetes 自体ではなく、ネットワーク管理者や Container Network Interface (CNI) によって設定される

### 11.4.1. What the network must be like

* Kubernetes では特定のネットワークテクノロジを使わなければならないという制限はないが、どのノードで Pod が動いているかに関わらず、Pod 同士が互いに通信できる必要がある。
* Pod から Pod に送信されたパケットは、送信元アドレスと宛先アドレスの両方を変更せずに相手側の Pod に到達する必要がある (NAT すべきではない)
* Pod 内で実行されているアプリケーションのネットワークが、同じネットワークスイッチに接続されているマシン上で実行されている場合と同じように動作させるために、これが重要。
* Pod・ノード間の通信も NAT-less でなければならない。

* ![11fig14_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig14_alt.jpg)

### 11.4.2. Diving deeper into how networking works

* Pod の IP アドレスやネットワーク名前空間はインフラストラクチャコンテナ (pause コンテナ) によって設定・保持されており、Pod 内のコンテナはそのネットワーク名前空間を使う。

* ![11fig15](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig15.jpg)

* インフラストラクチャコンテナが開始される前に、コンテナ用の仮想 NIC のペア (veth ペア) が作成される
* ペアの片方はホストのネットワーク名前空間にあり、ノードで ifconfig すると vethXXX のような形で見える
* もう片方はコンテナのネットワーク名前空間に移動され、eth0 にリネームされる
* ホスト側のネットワーク名前空間にある仮想 NIC はコンテナランタイムが使用するネットワークブリッジに接続されている。
* コンテナの eth0 インターフェイスには、ブリッジのアドレス範囲から IP アドレスが割り当てられている
* Pod 内のコンテナから eth0 にパケットを送信すると、eth0 -> vethXX -> ブリッジという流れで送信される (ブリッジに接続されちえるどの NIC でも受信することができる)
* 同一ノード上のコンテナはこれで互いに通信できるが、異なるノードのコンテナ間で通信を行えるようにするにはノード上のブリッジ同士を接続する必要がある
* ブリッジの接続にはオーバーレイネットワーク、アンダーレイネットワーク、または通常のレイヤ 3 ルーティングで実現可能

* ![11fig16_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig16_alt.jpg)

* 上の図にある通り、二つのノード間の Pod を通常のレイヤ 3 ルーティングで通信可能にするには、ノードの物理 NIC もブリッジに接続し、ノード A のルーティングテーブルにて 10.1.2.0/24 宛てのすべてのパケットがノード B にルーティングされるように設定する必要がある (逆も同様)。
* ノードが同じネットワークスイッチに接続されていて、その間にルータが存在しない場合には機能するが、そうでなければ Pod のプライベート IP を参照するのルータがパケットを落としてしまう。
* ルータの設定で対応することも可能だが、ノードが増えるにつれて設定ミスなどが発生しやすくなってしまう
* そのため、実際の基盤となるネットワークトポロジに関係なく、ノードが同じネットワークスイッチに接続されているように見えるソフトウェア定義ネットワーク (SDN) を使用する方が簡単

### 11.4.3. Introducing the Container Network Interface

**CNI について調べる**

## 11.5. HOW SERVICES ARE IMPLEMENTED

### 11.5.1. Introducing the kube-proxy

* サービスは固定 IP とポートを取得するためのもの
* 
* サービスに関連するものはすべて、各ノードで実行されている kube-proxy プロセスによって処理される
* この IP アドレスは仮想的なものであり、どの NIC にも割り当てられていないので、Pod からパケットが送信される際に送信元や宛先としてこの IP が見えることはない
* 重要な点: サービスは IP とポートのペアから構成されるものであり、サービス IP だけで何かを表すものではない (この IP に対して ping もできない)

### 11.5.2. How kube-proxy uses iptables

* API サーバーでサービスが作成されると仮想 IP がすぐに割り当てられ、その後ノードで稼働している kube-proxy agent にサービスが作成されたことが通知される
* それぞれの kube-proxy は、サービスの IP/ポートペア宛のパケットをインターセプトして、宛先アドレスが書き換えられるように iptables ルールを変更する
* これにより、パケットはサービス配下の Pod の一つにリダイレクトされるようになる
* kube-proxy は Endpoints オブジェクトの変更も監視している

**Endpoints について確認**

* kube-proxy によって、クライアントがサービスを介して Pod に接続される流れを示したのか下図。

* ![11fig17_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig17_alt.jpg)

## 11.6. RUNNING HIGHLY AVAILABLE CLUSTERS

### 11.6.1. Making your apps highly available

* Kubernetes では、様々なコントローラによって、ノードに障害が発生しても指定された数のアプリが実行され続けるに対処される
* アプリの可用性を高めるには、Deploymentリソースを介して実行し、適切な数のレプリカを構成するだけ。後のことは Kubernetes が面倒を見てくれる
* 水平スケールが可能でないアプリでこの恩恵を受けるためには、レプリカ数 1 で Deployment を作成する必要があるが、この場合関連するすべてのコントローラがノード障害に気づき、Pod レプリカを新たに作成してコンテナを起動する、という処理を行う必要があるため、短時間のダウンタイムが発生する
* ダウンタイムを回避するには、アクティブ・スタンバイ構成にして、リーダー選択メカニズムが必要になる
* リーダー選出をアプリ自体に組み込む必要はなく、サイドカーコンテナを使って実現可能。https://github.com/kubernetes/contrib/tree/master/election に例がある。
* このように Kubernetes が面倒を見てくれるので、アプリ自体の可用性を高めることは容易。では Kubernetes 自体はどうか？

### 11.6.2. Making Kubernetes Control Plane components highly available

* Kubernetes 自体の可用性を高めるには、マスターノードを複数構成にする必要がある。

* ![11fig18_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig18_alt.jpg)

* etcd は分散システムとして設計されているので、複数の etcd インスタンスを実行可能
* API サーバーはステートレスであり、これも水平スケール可能。一般的には etcd と同じインスタンスで API サーバーも起動し、ローカルの etcd に接続するようにする
* クライアント (kubectl だけでなくコントローラマネージャやスケジューラ、Kubelet も) はロードバランサ経由で API サーバーにアクセスするようにする
* 複数のコントローラマネージャやスケジューラを同時にアクティブにすると、それらが同時にクラスタの状態変化を検知して同じアクションを行い、競合が発生する可能性があるので望ましくない
* マスターノードを冗長化するときは、これらのコンポーネントはアクティブ・スタンバイ構成にする
* コントローラマネージャやスケジューラは API サーバーや etcd と同じインスタンスで実行することも、別のインスタンスで実行することも可能。同じインスタンスで実行している場合はローカルの API サーバーに直接アクセスできるが、別で実行している場合はロードバランサ経由でアクセスする

* ![11fig19_alt](https://learning.oreilly.com/library/view/kubernetes-in-action/9781617293726/11fig19_alt.jpg)

* リーダー選出は API サーバーにリソースを作成することで行なっている (コンポーネント間の通信は不要)
* 例えばスケジューラの場合
  * kube-system 名前空間 に kube-scheduler という名前の Endpoints リソースを作成
  * このリソースの control-plane.alpha.kubernetes.io/leader アノテーションの holderIdentity フィールドに自分の名前 (スケジューラが稼働しているノードの名前？) を設定できたスケジューラがリーダーとなる
  * リーダーは定期的に (デフォルトでは2秒ごとに) リソースを更新する (アノテーションの renewTime が更新されている)
  * この更新が行われなくなったことを他のスケジューラが検知すると、holderIdentity を自分の名前に書き換えてリーダーになろうとする

## 11.7. SUMMARY

* Kubernetes クラスタを構成するコンポーネントとそれらの役割
* API サーバーやスケジューラ、各種コントローラ、Kubelet がどのように動作するのか
* How the infrastructure container binds together all the containers of a pod
* ポッドが同一ノード内やノード間でどのように通信しているのか
* kube-proxy が iptables を使ってどのように Pod のロードバランシングを行なっているのか
* クラスタの HA を実現するためにコントロールプレーンのコンポーネントが複数のインスタンスでどのように動作しているのか
